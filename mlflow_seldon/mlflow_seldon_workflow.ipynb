{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow and Seldon\n",
    "\n",
    "### End to end example integrating MLFlow and Seldon, with A/B testing of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow](../images/mlflow_framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "This first section covers how to train models using MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLproject\n",
    "\n",
    "The MLproject file defines:\n",
    "- The environment where the training runs.\n",
    "- The hyperparameters that can be tweaked. In our case, these are $\\{\\alpha, l_{1}\\}$.\n",
    "- The interface to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mname\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m-\u001b[39;49;00m\u001b[34mtalk\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mconda_env\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mentry_points\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "  \u001b[34mmain\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mparameters\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "      \u001b[34malpha\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\r\n",
      "      \u001b[34ml1_ratio\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[31m{\u001b[39;49;00m\u001b[34mtype\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\u001b[31m,\u001b[39;49;00m \u001b[34mdefault\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.1\u001b[39;49;00m\u001b[31m}\u001b[39;49;00m\r\n",
      "    \u001b[34mcommand\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m\"python train.py {alpha} {l1_ratio}\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!ccat ./training/MLproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a single command to train the model. \n",
    "\n",
    "``` bash\n",
    "$ mlflow run ./training -P alpha=... -P l1_ratio=...\n",
    "```\n",
    "\n",
    "For our example, we will train two versions of the model, which we'll later compare using A/B testing.\n",
    "\n",
    "- $M_{1}$ with $\\alpha = 0.5$\n",
    "- $M_{2}$ with $\\alpha = 0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/13 16:06:20 INFO mlflow.projects: === Created directory /tmp/tmp65kd3by1 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/13 16:06:20 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.5 0.1' in run with ID '7d6024ced4fa4a23958e769993084a59' === \n",
      "Elasticnet model (alpha=0.500000, l1_ratio=0.100000):\n",
      "  RMSE: 0.7947931019036529\n",
      "  MAE: 0.6189130834228138\n",
      "  R2: 0.18411668718221819\n",
      "2019/11/13 16:06:22 INFO mlflow.projects: === Run (ID '7d6024ced4fa4a23958e769993084a59') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/04 18:52:21 INFO mlflow.projects: === Created directory /tmp/tmpv4thjgnr for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/04 18:52:21 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 1.0 0.1' in run with ID 'a3072d27f2cc40b990b9ff633c2c4131' === \n",
      "Elasticnet model (alpha=1.000000, l1_ratio=0.100000):\n",
      "  RMSE: 0.8107373707184711\n",
      "  MAE: 0.6241295925236751\n",
      "  R2: 0.15105362812007328\n",
      "2019/11/04 18:52:22 INFO mlflow.projects: === Run (ID 'a3072d27f2cc40b990b9ff633c2c4131') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLtrack\n",
    "\n",
    "The `train.py` script uses the `mlflow.log_param()` and `mlflow.log_metric()` commands to track each experiment. These are part of the `MLtrack` API, which tracks experiments parameters and results. These can be stored on a remote server, which can then be shared across the entire team. However, on our example we will store these locally on a `mlruns` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns\u001b[00m\r\n",
      "└── \u001b[01;34m0\u001b[00m\r\n",
      "    ├── \u001b[01;34m1ff63a6b537444df94458059bce313a7\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    ├── \u001b[01;34m7d6024ced4fa4a23958e769993084a59\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.gitRepoURL\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.git.repoURL\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    ├── \u001b[01;34ma3072d27f2cc40b990b9ff633c2c4131\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    └── meta.yaml\r\n",
      "\r\n",
      "19 directories, 51 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run `mlflow ui` to show these visually. This will start the MLflow server in http://localhost:5000.\n",
    "\n",
    "```bash\n",
    "$ mlflow ui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Starting gunicorn 19.9.0\n",
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Listening at: http://127.0.0.1:5000 (11409)\n",
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Using worker: sync\n",
      "[2019-11-04 18:52:34 +0530] [11412] [INFO] Booting worker with pid: 11412\n",
      "^C\n",
      "[2019-11-04 18:52:45 +0530] [11409] [INFO] Handling signal: int\n",
      "[2019-11-04 18:52:46 +0530] [11412] [INFO] Worker exiting (pid: 11412)\n"
     ]
    }
   ],
   "source": [
    "# !mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow UI](../images/mlflow-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLmodel\n",
    "\n",
    "The `MLmodel` file allows us to version and share models easily. Below we can see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34martifact_path\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\r\n",
      "\u001b[34mflavors\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "  \u001b[34mpython_function\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mdata\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\r\n",
      "    \u001b[34menv\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\r\n",
      "    \u001b[34mloader_module\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34msklearn\u001b[39;49;00m\r\n",
      "    \u001b[34mpython_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m3.6\u001b[39;49;00m\u001b[34m.9\u001b[39;49;00m\r\n",
      "  \u001b[34msklearn\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mpickled_model\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\r\n",
      "    \u001b[34mserialization_format\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mcloudpickle\u001b[39;49;00m\r\n",
      "    \u001b[34msklearn_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.19\u001b[39;49;00m\u001b[34m.1\u001b[39;49;00m\r\n",
      "\u001b[34mrun_id\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34ma3072d27f2cc40b990b9ff633c2c4131\u001b[39;49;00m\r\n",
      "\u001b[34mutc_time_created\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m'2019-11-04 13:22:22.495857'\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!ccat ./mlruns/0/a3072d27f2cc40b990b9ff633c2c4131/artifacts/model/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the `MLmodel` keeps track, between others, of\n",
    "\n",
    "- The experiment id, `a3072d27f2cc40b990b9ff633c2c4131`\n",
    "- Date \n",
    "- Version of `sklearn` \n",
    "- How the model was stored\n",
    "\n",
    "As we shall see shortly, the pre-packaged Seldon's model server will use this file to serve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Serving\n",
    "\n",
    "### To serve this model we will use Seldon.\n",
    "### Seldon Core is an open source platform for deploying machine learning models on a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon](../images/seldon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Set up\n",
    "\n",
    "Before anything, we will first set up the `k8s` cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create k8s cluster\n",
    "\n",
    "We will create a local cluster using [kind](https://kind.sigs.k8s.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"kind\" ...\n",
      " ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n",
      " ✓ Preparing nodes 📦 \n",
      " ✓ Creating kubeadm config 📜 \n",
      " ✓ Starting control-plane 🕹️ \n",
      " ✓ Installing CNI 🔌 \n",
      " ✓ Installing StorageClass 💾 \n",
      "Cluster creation complete. You can now use the cluster with:\n",
      "\n",
      "export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster\n",
    "# !export KUBECONFIG=\"$(kind get kubeconfig-path --name=kind)\"\n",
    "# !kind delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/seldon created\n",
      "Context \"kubernetes-admin@kind\" modified.\n"
     ]
    }
   ],
   "source": [
    "!kubectl create namespace seldon\n",
    "!kubectl config set-context $(kubectl config current-context) --namespace=seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io/kube-system-cluster-admin created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create clusterrolebinding kube-system-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then install Helm and a corresponding service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/tiller created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/tiller created\n",
      "$HELM_HOME has been configured at /home/akash/.helm.\n",
      "\n",
      "Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n",
      "\n",
      "Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\n",
      "To prevent this, run `helm init` with the --tiller-tls-verify flag.\n",
      "For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n",
      "Happy Helming!\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kube-system create sa tiller\n",
    "!kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n",
    "!helm init --service-account tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"tiller-deploy\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"tiller-deploy\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/tiller-deploy -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind\n",
      "/home/akash/.kube/kind-config-kind\n",
      "\u001b[0;32mKubernetes master\u001b[0m is running at \u001b[0;33mhttps://127.0.0.1:41081\u001b[0m\n",
      "\u001b[0;32mKubeDNS\u001b[0m is running at \u001b[0;33mhttps://127.0.0.1:41081/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\u001b[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "$HELM_HOME has been configured at /home/akash/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n"
     ]
    }
   ],
   "source": [
    "!kind get clusters\n",
    "!echo $KUBECONFIG\n",
    "!kubectl cluster-info\n",
    "!helm init --history-max 200\n",
    "# !kubectl rollout status deploy/tiller-deploy -n kube-system\n",
    "# !kubectl create serviceaccount --namespace kube-system tiller\n",
    "# !kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n",
    "# !kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install `seldon-core` on the new cluster using `helm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core\n",
      "LAST DEPLOYED: Wed Nov 13 18:42:47 2019\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Role\n",
      "NAME                         AGE\n",
      "seldon-leader-election-role  0s\n",
      "seldon-manager-cm-role       0s\n",
      "\n",
      "==> v1/Deployment\n",
      "NAME                       DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "seldon-controller-manager  1        1        1           0          0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME            SECRETS  AGE\n",
      "seldon-manager  1        0s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                         AGE\n",
      "seldondeployments.machinelearning.seldon.io  0s\n",
      "\n",
      "==> v1/ClusterRole\n",
      "seldon-manager-css-role  0s\n",
      "seldon-manager-role      0s\n",
      "seldon-manager-sas-role  0s\n",
      "seldon-proxy-role        0s\n",
      "\n",
      "==> v1/ClusterRoleBinding\n",
      "NAME                            AGE\n",
      "seldon-manager-css-rolebinding  0s\n",
      "seldon-manager-rolebinding      0s\n",
      "seldon-manager-sas-rolebinding  0s\n",
      "seldon-proxy-rolebinding        0s\n",
      "\n",
      "==> v1/RoleBinding\n",
      "NAME                                AGE\n",
      "seldon-leader-election-rolebinding  0s\n",
      "seldon-manager-cm-rolebinding       0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                                       TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE\n",
      "seldon-controller-manager-metrics-service  ClusterIP  10.100.33.117  <none>       8443/TCP  0s\n",
      "seldon-webhook-service                     ClusterIP  10.108.241.84  <none>       443/TCP   0s\n",
      "\n",
      "==> v1beta1/MutatingWebhookConfiguration\n",
      "NAME                                   AGE\n",
      "seldon-mutating-webhook-configuration  0s\n",
      "\n",
      "==> v1beta1/ValidatingWebhookConfiguration\n",
      "seldon-validating-webhook-configuration  0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                        TYPE               DATA  AGE\n",
      "seldon-webhook-server-cert  kubernetes.io/tls  3     0s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME           DATA  AGE\n",
      "seldon-config  1     0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                        READY  STATUS             RESTARTS  AGE\n",
      "seldon-controller-manager-85d65bbcb7-dhdcq  0/1    ContainerCreating  0         0s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install \\\n",
    "    seldon-core-operator \\\n",
    "    --name seldon-core \\\n",
    "    --repo https://storage.googleapis.com/seldon-charts \\\n",
    "    --namespace seldon-system \\\n",
    "    --set usagemetrics.enabled=true \\\n",
    "    --set ambassador.enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"seldon-controller-manager\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"seldon-controller-manager\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/seldon-controller-manager -n seldon-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we install `ambassador` which will allow us to reach the Seldon engine in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   ambassador\n",
      "LAST DEPLOYED: Wed Nov 13 18:43:41 2019\n",
      "NAMESPACE: seldon\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "ambassador  0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME               TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE\n",
      "ambassador-admins  ClusterIP     10.101.122.99  <none>       8877/TCP                    0s\n",
      "ambassador         LoadBalancer  10.102.36.166  <pending>    80:30649/TCP,443:31205/TCP  0s\n",
      "\n",
      "==> v1/Deployment\n",
      "NAME        DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "ambassador  3        3        3           0          0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                         READY  STATUS             RESTARTS  AGE\n",
      "ambassador-79744f49fd-9w5pt  0/1    ContainerCreating  0         0s\n",
      "ambassador-79744f49fd-wnnvz  0/1    ContainerCreating  0         0s\n",
      "ambassador-79744f49fd-z4dcl  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "ambassador  1        1s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                          AGE\n",
      "consulresolvers.getambassador.io              1s\n",
      "tcpmappings.getambassador.io                  1s\n",
      "mappings.getambassador.io                     1s\n",
      "kubernetesserviceresolvers.getambassador.io   1s\n",
      "kubernetesendpointresolvers.getambassador.io  1s\n",
      "modules.getambassador.io                      0s\n",
      "ratelimitservices.getambassador.io            0s\n",
      "tracingservices.getambassador.io              0s\n",
      "authservices.getambassador.io                 0s\n",
      "tlscontexts.getambassador.io                  0s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "ambassador  0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "Congratuations! You've successfully installed Ambassador.\n",
      "\n",
      "For help, visit our Slack at https://d6e.co/slack or view the documentation online at https://www.getambassador.io.\n",
      "\n",
      "To get the IP address of Ambassador, run the following commands:\n",
      "NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n",
      "     You can watch the status of by running 'kubectl get svc -w  --namespace seldon ambassador'\n",
      "\n",
      "  On GKE/Azure:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace seldon ambassador -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
      "\n",
      "  On AWS:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace seldon ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
      "\n",
      "  echo http://$SERVICE_IP:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install stable/ambassador --name ambassador --set crds.keep=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"ambassador\" rollout to finish: 0 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 1 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 2 of 3 updated replicas are available...\n",
      "deployment \"ambassador\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deployment.apps/ambassador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward port\n",
    "\n",
    "Once the cluster has been created, we need to allow access from the outside to the `ambassador` gateway.\n",
    "One way to do this is to use the `kubectl port-forward` command.\n",
    "In particular, we will forward port `8003` of our local host to the cluster's gateway.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "kubectl \\\n",
    "    port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    8003:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Seldon Core Analytics\n",
    "\n",
    "Later, after we deploy the models, we will compare their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "For that part to work, we first need to install Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core-analytics\n",
      "LAST DEPLOYED: Wed Nov 13 19:16:27 2019\n",
      "NAMESPACE: seldon\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                                      READY  STATUS             RESTARTS  AGE\n",
      "grafana-prom-import-dashboards-d867r      0/1    ContainerCreating  0         1s\n",
      "alertmanager-deployment-56c4cb6977-86rmk  0/1    ContainerCreating  0         1s\n",
      "grafana-prom-deployment-8564b575dd-kgk6g  0/1    ContainerCreating  0         0s\n",
      "prometheus-node-exporter-tkw6t            0/1    ContainerCreating  0         0s\n",
      "prometheus-deployment-847fdcf987-lcrvp    0/1    Pending            0         0s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1/Job\n",
      "NAME                            DESIRED  SUCCESSFUL  AGE\n",
      "grafana-prom-import-dashboards  1        0           1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "alertmanager-deployment  1        1        1           0          1s\n",
      "grafana-prom-deployment  1        1        1           0          0s\n",
      "prometheus-deployment    1        1        1           0          0s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)       AGE\n",
      "alertmanager              ClusterIP  10.107.224.49   <none>       80/TCP        1s\n",
      "grafana-prom              NodePort   10.111.104.114  <none>       80:30977/TCP  0s\n",
      "prometheus-node-exporter  ClusterIP  None            <none>       9100/TCP      0s\n",
      "prometheus-seldon         ClusterIP  10.98.237.144   <none>       80/TCP        0s\n",
      "\n",
      "==> v1beta1/DaemonSet\n",
      "NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\n",
      "prometheus-node-exporter  1        1        0      1           0          <none>         0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                 TYPE    DATA  AGE\n",
      "grafana-prom-secret  Opaque  1     1s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME                       DATA  AGE\n",
      "alertmanager-server-conf   1     1s\n",
      "grafana-import-dashboards  11    1s\n",
      "prometheus-rules           0     1s\n",
      "prometheus-server-conf     1     1s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "prometheus  1        1s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-analytics --name seldon-core-analytics \\\n",
    "     --repo https://storage.googleapis.com/seldon-charts \\\n",
    "     --set grafana_prom_admin_password=password \\\n",
    "     --set persistence.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Grafana, it will be necessary to forward the port to the respective pod as we did previously to access the Seldon Core deployment.\n",
    "The credentials will be simply `admin` // `password`.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ kubectl port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app=grafana-prom-server -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    3000:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Deploy models\n",
    "\n",
    "Once the cluster is set up, the next step will be to upload these models into a common repository and to deploy two `SeldonDeployment` specs to `k8s`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload models (optional)\n",
    "\n",
    "To make sure our `k8s` pods have access to the models we have just trained using `MLflow`, we will upload them into Google Cloud Storage. Note that to run these commands you need write access into the `gs://seldon-models` bucket and you need to have `gsutil` set up.\n",
    "\n",
    "We will upload both versions of the model to:\n",
    "\n",
    "- `gs://seldon-models/mlflow/model-a`\n",
    "- `gs://seldon-models/mlflow/model-b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://seldon-models-dhs/\r\n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp -r mlruns/0/a3072d27f2cc40b990b9ff633c2c4131/artifacts/model gs://seldon-models-dhs/mlflow/model-a\n",
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy specs\n",
    "\n",
    "We will deploy our A/B inference graph to our `k8s` cluster. As we can see below, we will route 50% of the traffic to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: model-a\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: model-a\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models-dhs/mlflow/model-a\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: default\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "seldondeployment.machinelearning.seldon.io/model-a created\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./serving/model-a.yaml\n",
    "!kubectl apply -f ./serving/model-a.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify these have been deployed by checking the pods and `SeldonDeployment` resources in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                       READY   STATUS    RESTARTS   AGE\r\n",
      "ambassador-79744f49fd-9w5pt                1/1     Running   0          9m12s\r\n",
      "ambassador-79744f49fd-wnnvz                1/1     Running   0          9m12s\r\n",
      "ambassador-79744f49fd-z4dcl                1/1     Running   0          9m12s\r\n",
      "model-a-default-77efeb1-7f687c5b8b-hjts2   2/2     Running   0          6m8s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "ambassador                3/3     3            3           4m8s\r\n",
      "model-a-default-77efeb1   0/1     1            0           64s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"model-a-default-77efeb1\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"model-a-default-77efeb1\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/model-a-default-77efeb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      AGE\r\n",
      "model-a   6m9s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get sdep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models\n",
    "\n",
    "We will now run a sample query to test that the inference graph is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "sc = SeldonClient(deployment_name=\"model-a\",namespace=\"seldon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  tensor {\n",
      "    shape: 1\n",
      "    shape: 11\n",
      "    values: 0.679423648454573\n",
      "    values: 0.40174071357426766\n",
      "    values: 0.4907872244252788\n",
      "    values: 0.9652199625809575\n",
      "    values: 0.35011405617402425\n",
      "    values: 0.5314154532123815\n",
      "    values: 0.9085835710372856\n",
      "    values: 0.14433139025482644\n",
      "    values: 0.2971222340126688\n",
      "    values: 0.09946862715362115\n",
      "    values: 0.8433473381558532\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "  puid: \"36f0h6m1tss6j5aumep1i4019f\"\n",
      "  requestPath {\n",
      "    key: \"wines-classifier\"\n",
      "    value: \"seldonio/mlflowserver_rest:0.2\"\n",
      "  }\n",
      "}\n",
      "data {\n",
      "  tensor {\n",
      "    shape: 1\n",
      "    values: 4.792492792022103\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = sc.predict(gateway=\"ambassador\",transport=\"rest\",shape=(1,11))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "!http \\\n",
    "    --print b \\\n",
    "    localhost:8003/seldon/default/wines-classifier/api/v0.1/predictions \\\n",
    "    data:='{\\\n",
    "        \"names\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"], \\\n",
    "        \"ndarray\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics\n",
    "\n",
    "Now that we have both models running in production, we can analyse their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "To do so, we will iterate over the training set (which can be foud in `./training/wine-quality.csv`), making a request and sending the feedback of the prediction.\n",
    "\n",
    "Since the `/feedback` endpoint requires a `reward` signal (i.e. higher better), we will simulate one as\n",
    "\n",
    "$$\n",
    "  R(x_{n})\n",
    "    = \\begin{cases}\n",
    "        \\frac{1}{(y_{n} - f(x_{n}))^{2}} &, y_{n} \\neq f(x_{n}) \\\\\n",
    "        500 &, y_{n} = f(x_{n})\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    ", where $R(x_{n})$ is the reward for input point $x_{n}$, $f(x_{n})$ is our trained model and $y_{n}$ is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [6.292674122292678]\n",
       "1       [5.3391608345408565]\n",
       "2       [264.15460719776115]\n",
       "3       [11.500813676302768]\n",
       "4       [11.500813676302768]\n",
       "                ...         \n",
       "4893    [339.87306395634363]\n",
       "4894     [1.454915963262557]\n",
       "4895    [21.788327812948797]\n",
       "4896    [1.4281815246963911]\n",
       "4897    [114.20670836019673]\n",
       "Length: 4898, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    namespace=\"seldon\",\n",
    "    deployment_name='model-a')\n",
    "\n",
    "df = pd.read_csv(\"../data/wine-quality.csv\")\n",
    "\n",
    "def _get_reward(y, y_pred):\n",
    "    if y == y_pred:\n",
    "        return 500    \n",
    "    \n",
    "    return 1 / np.square(y - y_pred)\n",
    "\n",
    "def _test_row(row):\n",
    "    input_features = row[:-1]\n",
    "    feature_names = input_features.index.to_list()\n",
    "    X = input_features.values.reshape(1, -1)\n",
    "    y = row[-1].reshape(1, -1)\n",
    "    \n",
    "    r = sc.predict(\n",
    "        data=X,\n",
    "        names=feature_names)\n",
    "    \n",
    "    y_pred = r.response.data.tensor.values\n",
    "    reward = _get_reward(y, y_pred)\n",
    "    sc.feedback(\n",
    "        prediction_request=r.request,\n",
    "        prediction_response=r.response,\n",
    "        reward=reward)\n",
    "    \n",
    "    return reward[0]\n",
    "\n",
    "df.apply(_test_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can now access the Grafana dashboard in http://localhost:3000 (credentials are `admin` // `password`). Inside the portal, we will go to the Prediction Analytics dashboard.\n",
    " \n",
    " \n",
    "We can see a snapshot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon Analytics](../images/seldon-analytics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
