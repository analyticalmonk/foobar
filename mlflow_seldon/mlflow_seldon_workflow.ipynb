{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow and Seldon\n",
    "\n",
    "### End to end example integrating MLFlow and Seldon, with A/B testing of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow](../images/mlflow_framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "This first section covers how to train models using MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLproject\n",
    "\n",
    "The MLproject file defines:\n",
    "- The environment where the training runs.\n",
    "- The hyperparameters that can be tweaked. In our case, these are $\\{\\alpha, l_{1}\\}$.\n",
    "- The interface to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mname\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m-\u001b[39;49;00m\u001b[34mtalk\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mconda_env\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mentry_points\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "  \u001b[34mmain\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mparameters\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "      \u001b[34malpha\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\r\n",
      "      \u001b[34ml1_ratio\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[31m{\u001b[39;49;00m\u001b[34mtype\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\u001b[31m,\u001b[39;49;00m \u001b[34mdefault\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.1\u001b[39;49;00m\u001b[31m}\u001b[39;49;00m\r\n",
      "    \u001b[34mcommand\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m\"python train.py {alpha} {l1_ratio}\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!ccat ./training/MLproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a single command to train the model. \n",
    "\n",
    "``` bash\n",
    "$ mlflow run ./training -P alpha=... -P l1_ratio=...\n",
    "```\n",
    "\n",
    "For our example, we will train two versions of the model, which we'll later compare using A/B testing.\n",
    "\n",
    "- $M_{1}$ with $\\alpha = 0.5$\n",
    "- $M_{2}$ with $\\alpha = 0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/04 18:52:16 INFO mlflow.projects: === Created directory /tmp/tmpc5flveqy for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/04 18:52:16 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.5 0.1' in run with ID '1ff63a6b537444df94458059bce313a7' === \n",
      "Elasticnet model (alpha=0.500000, l1_ratio=0.100000):\n",
      "  RMSE: 0.7947931019036529\n",
      "  MAE: 0.6189130834228138\n",
      "  R2: 0.18411668718221819\n",
      "2019/11/04 18:52:17 INFO mlflow.projects: === Run (ID '1ff63a6b537444df94458059bce313a7') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/04 18:52:21 INFO mlflow.projects: === Created directory /tmp/tmpv4thjgnr for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/04 18:52:21 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 1.0 0.1' in run with ID 'a3072d27f2cc40b990b9ff633c2c4131' === \n",
      "Elasticnet model (alpha=1.000000, l1_ratio=0.100000):\n",
      "  RMSE: 0.8107373707184711\n",
      "  MAE: 0.6241295925236751\n",
      "  R2: 0.15105362812007328\n",
      "2019/11/04 18:52:22 INFO mlflow.projects: === Run (ID 'a3072d27f2cc40b990b9ff633c2c4131') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLtrack\n",
    "\n",
    "The `train.py` script uses the `mlflow.log_param()` and `mlflow.log_metric()` commands to track each experiment. These are part of the `MLtrack` API, which tracks experiments parameters and results. These can be stored on a remote server, which can then be shared across the entire team. However, on our example we will store these locally on a `mlruns` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns\u001b[00m\r\n",
      "└── \u001b[01;34m0\u001b[00m\r\n",
      "    ├── \u001b[01;34m1ff63a6b537444df94458059bce313a7\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    ├── \u001b[01;34ma3072d27f2cc40b990b9ff633c2c4131\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    └── meta.yaml\r\n",
      "\r\n",
      "13 directories, 33 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run `mlflow ui` to show these visually. This will start the MLflow server in http://localhost:5000.\n",
    "\n",
    "```bash\n",
    "$ mlflow ui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Starting gunicorn 19.9.0\n",
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Listening at: http://127.0.0.1:5000 (11409)\n",
      "[2019-11-04 18:52:34 +0530] [11409] [INFO] Using worker: sync\n",
      "[2019-11-04 18:52:34 +0530] [11412] [INFO] Booting worker with pid: 11412\n",
      "^C\n",
      "[2019-11-04 18:52:45 +0530] [11409] [INFO] Handling signal: int\n",
      "[2019-11-04 18:52:46 +0530] [11412] [INFO] Worker exiting (pid: 11412)\n"
     ]
    }
   ],
   "source": [
    "# !mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow UI](../images/mlflow-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLmodel\n",
    "\n",
    "The `MLmodel` file allows us to version and share models easily. Below we can see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34martifact_path\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\r\n",
      "\u001b[34mflavors\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "  \u001b[34mpython_function\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mdata\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\r\n",
      "    \u001b[34menv\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\r\n",
      "    \u001b[34mloader_module\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34msklearn\u001b[39;49;00m\r\n",
      "    \u001b[34mpython_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m3.6\u001b[39;49;00m\u001b[34m.9\u001b[39;49;00m\r\n",
      "  \u001b[34msklearn\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mpickled_model\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmodel\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34mpkl\u001b[39;49;00m\r\n",
      "    \u001b[34mserialization_format\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mcloudpickle\u001b[39;49;00m\r\n",
      "    \u001b[34msklearn_version\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.19\u001b[39;49;00m\u001b[34m.1\u001b[39;49;00m\r\n",
      "\u001b[34mrun_id\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34ma3072d27f2cc40b990b9ff633c2c4131\u001b[39;49;00m\r\n",
      "\u001b[34mutc_time_created\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m'2019-11-04 13:22:22.495857'\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!ccat ./mlruns/0/a3072d27f2cc40b990b9ff633c2c4131/artifacts/model/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the `MLmodel` keeps track, between others, of\n",
    "\n",
    "- The experiment id, `a3072d27f2cc40b990b9ff633c2c4131`\n",
    "- Date \n",
    "- Version of `sklearn` \n",
    "- How the model was stored\n",
    "\n",
    "As we shall see shortly, the pre-packaged Seldon's model server will use this file to serve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Serving\n",
    "\n",
    "### To serve this model we will use Seldon.\n",
    "### Seldon Core is an open source platform for deploying machine learning models on a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon](../images/seldon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Set up\n",
    "\n",
    "Before anything, we will first set up the `k8s` cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create k8s cluster\n",
    "\n",
    "Firstly, we will create a cluster using [kind](https://kind.sigs.k8s.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"kind\" ...\n",
      " ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n",
      " ✓ Preparing nodes 📦 \n",
      " ✓ Creating kubeadm config 📜 \n",
      " ✓ Starting control-plane 🕹️ \n",
      " ✓ Installing CNI 🔌 \n",
      " ✓ Installing StorageClass 💾 \n",
      "Cluster creation complete. You can now use the cluster with:\n",
      "\n",
      "export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster\n",
    "!export KUBECONFIG=\"$(kind get kubeconfig-path --name=kind)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then install Helm and a corresponding service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kind get clusters\n",
    "# !export KUBECONFIG=\"$(kind get kubeconfig-path)\"\n",
    "# !echo $KUBECONFIG\n",
    "# !kubectl cluster-info\n",
    "# !helm init --history-max 200\n",
    "# !kubectl rollout status deploy/tiller-deploy -n kube-system\n",
    "# !kubectl create serviceaccount --namespace kube-system tiller\n",
    "# !kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n",
    "# !kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install `seldon-core` on the new cluster using `helm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm install \\\n",
    "#     seldon-core-operator \\\n",
    "#     --name seldon-core \\\n",
    "#     --repo https://storage.googleapis.com/seldon-charts \\\n",
    "#     --namespace seldon-system \\\n",
    "#     --set usagemetrics.enabled=true \\\n",
    "#     --set ambassador.enabled=true\n",
    "# !kubectl rollout status statefulset.apps/seldon-operator-controller-manager -n seldon-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we install `ambassador` which will allow us to reach the Seldon engine in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm install stable/ambassador --name ambassador --set crds.keep=false\n",
    "# !kubectl rollout status deployment.apps/ambassador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward port\n",
    "\n",
    "Once the cluster has been created, we need to allow access from the outside to the `ambassador` gateway.\n",
    "One way to do this is to use the `kubectl port-forward` command.\n",
    "In particular, we will forward port `8003` of our local host to the cluster's gateway.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "kubectl \\\n",
    "    port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    8003:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Seldon Core Analytics\n",
    "\n",
    "Later, after we deploy the models, we will compare their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "For that part to work, we first need to install Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core-analytics\n",
      "LAST DEPLOYED: Tue Oct 29 23:27:40 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Job\n",
      "NAME                            DESIRED  SUCCESSFUL  AGE\n",
      "grafana-prom-import-dashboards  1        0           1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "alertmanager-deployment  1        1        1           0          1s\n",
      "grafana-prom-deployment  1        1        1           0          1s\n",
      "prometheus-deployment    1        1        1           0          1s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)       AGE\n",
      "alertmanager              ClusterIP  10.110.189.250  <none>       80/TCP        1s\n",
      "grafana-prom              NodePort   10.100.241.163  <none>       80:31194/TCP  1s\n",
      "prometheus-node-exporter  ClusterIP  None            <none>       9100/TCP      1s\n",
      "prometheus-seldon         ClusterIP  10.101.114.68   <none>       80/TCP        1s\n",
      "\n",
      "==> v1beta1/DaemonSet\n",
      "NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\n",
      "prometheus-node-exporter  1        1        0      1           0          <none>         1s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                      READY  STATUS             RESTARTS  AGE\n",
      "grafana-prom-import-dashboards-5txg5      0/1    ContainerCreating  0         1s\n",
      "alertmanager-deployment-db58649dd-9vpzg   0/1    ContainerCreating  0         1s\n",
      "grafana-prom-deployment-8564b575dd-fgkwd  0/1    ContainerCreating  0         1s\n",
      "prometheus-node-exporter-x9xkw            0/1    Pending            0         1s\n",
      "prometheus-deployment-d57b5c748-wb7lz     0/1    Pending            0         1s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                 TYPE    DATA  AGE\n",
      "grafana-prom-secret  Opaque  1     1s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME                       DATA  AGE\n",
      "alertmanager-server-conf   1     1s\n",
      "grafana-import-dashboards  11    1s\n",
      "prometheus-rules           0     1s\n",
      "prometheus-server-conf     1     1s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "prometheus  1        1s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-analytics --name seldon-core-analytics \\\n",
    "     --repo https://storage.googleapis.com/seldon-charts \\\n",
    "     --set grafana_prom_admin_password=password \\\n",
    "     --set persistence.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Grafana, it will be necessary to forward the port to the respective pod as we did previously to access the Seldon Core deployment.\n",
    "The credentials will be simply `admin` // `password`.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ kubectl port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app=grafana-prom-server -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    3000:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Deploy models\n",
    "\n",
    "Once the cluster is set up, the next step will to upload these models into a common repository and to deploy two `SeldonDeployment` specs to `k8s`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload models (optional)\n",
    "\n",
    "To make sure our `k8s` pods have access to the models we have just trained using `MLflow`, we will upload them into Google Cloud Storage. Note that to run these commands you need write access into the `gs://seldon-models` bucket and you need to have `gsutil` set up.\n",
    "\n",
    "We will upload both versions of the model to:\n",
    "\n",
    "- `gs://seldon-models/mlflow/model-a`\n",
    "- `gs://seldon-models/mlflow/model-b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp -r mlruns/0/a3072d27f2cc40b990b9ff633c2c4131/artifacts/model gs://seldon-models/mlflow/model-a\n",
    "# !gsutil cp -r mlruns/0/1ff63a6b537444df94458059bce313a7/artifacts/model gs://seldon-models/mlflow/model-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy specs\n",
    "\n",
    "We will deploy our A/B inference graph to our `k8s` cluster. As we can see below, we will route 50% of the traffic to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models-avdhs/mlflow/model-a\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-a\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models-avdhs/mlflow/model-b\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-b\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "seldondeployment.machinelearning.seldon.io/wines-classifier created\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./serving/model-a-b.yaml\n",
    "!kubectl apply -f ./serving/model-a-b.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify these have been deployed by checking the pods and `SeldonDeployment` resources in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                READY   STATUS      RESTARTS   AGE\r\n",
      "alertmanager-deployment-db58649dd-9vpzg             1/1     Running     0          5d19h\r\n",
      "ambassador-79744f49fd-d8rqj                         1/1     Running     2          5d19h\r\n",
      "ambassador-79744f49fd-rsvl8                         1/1     Running     0          5d19h\r\n",
      "ambassador-79744f49fd-rvr2f                         1/1     Running     1          5d19h\r\n",
      "grafana-prom-deployment-8564b575dd-fgkwd            1/1     Running     0          5d19h\r\n",
      "grafana-prom-import-dashboards-5txg5                0/1     Completed   0          5d19h\r\n",
      "prometheus-deployment-d57b5c748-wb7lz               1/1     Running     0          5d19h\r\n",
      "prometheus-node-exporter-x9xkw                      1/1     Running     0          5d19h\r\n",
      "wines-classifier-model-a-77efeb1-556d9fd4d6-mzz8c   0/2     Init:0/1    0          19s\r\n",
      "wines-classifier-model-b-77efeb1-5597885566-n52z7   0/2     Init:0/1    0          19s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               AGE\r\n",
      "wines-classifier   21s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get sdep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models\n",
    "\n",
    "We will now run a sample query to test that the inference graph is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!http \\\n",
    "    --print b \\\n",
    "    localhost:8003/seldon/default/wines-classifier/api/v0.1/predictions \\\n",
    "    data:='{\\\n",
    "        \"names\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"], \\\n",
    "        \"ndarray\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics\n",
    "\n",
    "Now that we have both models running in production, we can analyse their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "To do so, we will iterate over the training set (which can be foud in `./training/wine-quality.csv`), making a request and sending the feedback of the prediction.\n",
    "\n",
    "Since the `/feedback` endpoint requires a `reward` signal (i.e. higher better), we will simulate one as\n",
    "\n",
    "$$\n",
    "  R(x_{n})\n",
    "    = \\begin{cases}\n",
    "        \\frac{1}{(y_{n} - f(x_{n}))^{2}} &, y_{n} \\neq f(x_{n}) \\\\\n",
    "        500 &, y_{n} = f(x_{n})\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    ", where $R(x_{n})$ is the reward for input point $x_{n}$, $f(x_{n})$ is our trained model and $y_{n}$ is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    namespace=\"default\",\n",
    "    deployment_name='wines-classifier')\n",
    "\n",
    "df = pd.read_csv(\"./training/wine-quality.csv\")\n",
    "\n",
    "def _get_reward(y, y_pred):\n",
    "    if y == y_pred:\n",
    "        return 500    \n",
    "    \n",
    "    return 1 / np.square(y - y_pred)\n",
    "\n",
    "def _test_row(row):\n",
    "    input_features = row[:-1]\n",
    "    feature_names = input_features.index.to_list()\n",
    "    X = input_features.values.reshape(1, -1)\n",
    "    y = row[-1].reshape(1, -1)\n",
    "    \n",
    "    r = sc.predict(\n",
    "        data=X,\n",
    "        names=feature_names)\n",
    "    \n",
    "    y_pred = r.response.data.tensor.values\n",
    "    reward = _get_reward(y, y_pred)\n",
    "    sc.feedback(\n",
    "        prediction_request=r.request,\n",
    "        prediction_response=r.response,\n",
    "        reward=reward)\n",
    "    \n",
    "    return reward[0]\n",
    "\n",
    "df.apply(_test_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can now access the Grafana dashboard in http://localhost:3000 (credentials are `admin` // `password`). Inside the portal, we will go to the Prediction Analytics dashboard.\n",
    " \n",
    " \n",
    "We can see a snapshot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon Analytics](../images/seldon-analytics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
