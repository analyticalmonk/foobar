{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow and Seldon\n",
    "\n",
    "End to end example integrating MLFlow and Seldon, with A/B testing of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "This first section covers how to train models using MLFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLproject\n",
    "\n",
    "The MLproject file defines:\n",
    "- The environment where the training runs.\n",
    "- The hyperparameters that can be tweaked. In our case, these are $\\{\\alpha, l_{1}\\}$.\n",
    "- The interface to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mname\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mmlflow\u001b[39;49;00m\u001b[31m-\u001b[39;49;00m\u001b[34mtalk\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mconda_env\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mconda\u001b[39;49;00m\u001b[31m.\u001b[39;49;00m\u001b[34myaml\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mentry_points\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "  \u001b[34mmain\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "    \u001b[34mparameters\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m\r\n",
      "      \u001b[34malpha\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\r\n",
      "      \u001b[34ml1_ratio\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[31m{\u001b[39;49;00m\u001b[34mtype\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34mfloat\u001b[39;49;00m\u001b[31m,\u001b[39;49;00m \u001b[34mdefault\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[34m0.1\u001b[39;49;00m\u001b[31m}\u001b[39;49;00m\r\n",
      "    \u001b[34mcommand\u001b[39;49;00m\u001b[31m:\u001b[39;49;00m \u001b[33m\"python train.py {alpha} {l1_ratio}\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!ccat ./training/MLproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a single command to train the model. \n",
    "\n",
    "``` bash\n",
    "$ mlflow run ./training -P alpha=... -P l1_ratio=...\n",
    "```\n",
    "\n",
    "For our example, we will train two versions of the model, which we'll later compare using A/B testing.\n",
    "\n",
    "- $M_{1}$ with $\\alpha = 0.5$\n",
    "- $M_{2}$ with $\\alpha = 0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/04 14:27:02 INFO mlflow.projects: === Created directory /tmp/tmp3ynwn0d2 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/04 14:27:02 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.5 0.1' in run with ID '7440adcc4c3144aeb9bd234b527565ae' === \n",
      "Elasticnet model (alpha=0.500000, l1_ratio=0.100000):\n",
      "  RMSE: 0.7947931019036529\n",
      "  MAE: 0.6189130834228138\n",
      "  R2: 0.18411668718221819\n",
      "2019/11/04 14:27:03 INFO mlflow.projects: === Run (ID '7440adcc4c3144aeb9bd234b527565ae') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/10/29 22:50:22 INFO mlflow.projects: === Created directory /tmp/tmp2fcxg01g for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/10/29 22:50:22 INFO mlflow.projects: === Running command 'source /home/akash/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 1.0 0.1' in run with ID '539b7e5225844c88a764baaa10112f00' === \n",
      "Elasticnet model (alpha=1.000000, l1_ratio=0.100000):\n",
      "  RMSE: 0.8107373707184711\n",
      "  MAE: 0.6241295925236751\n",
      "  R2: 0.15105362812007328\n",
      "2019/10/29 22:50:24 INFO mlflow.projects: === Run (ID '539b7e5225844c88a764baaa10112f00') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run ./training -P alpha=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLtrack\n",
    "\n",
    "The `train.py` script uses the `mlflow.log_param()` and `mlflow.log_metric()` commands to track each experiment. These are part of the `MLtrack` API, which tracks experiments parameters and results. These can be stored on a remote server, which can then be shared across the entire team. However, on our example we will store these locally on a `mlruns` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns\u001b[00m\r\n",
      "└── \u001b[01;34m0\u001b[00m\r\n",
      "    ├── \u001b[01;34m7440adcc4c3144aeb9bd234b527565ae\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   │   └── \u001b[01;34mmodel\u001b[00m\r\n",
      "    │   │       ├── conda.yaml\r\n",
      "    │   │       ├── MLmodel\r\n",
      "    │   │       └── model.pkl\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   │   ├── mae\r\n",
      "    │   │   ├── r2\r\n",
      "    │   │   └── rmse\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    ├── \u001b[01;34mae803fa1a775416e82f4b4be2452c70e\u001b[00m\r\n",
      "    │   ├── \u001b[01;34martifacts\u001b[00m\r\n",
      "    │   ├── meta.yaml\r\n",
      "    │   ├── \u001b[01;34mmetrics\u001b[00m\r\n",
      "    │   ├── \u001b[01;34mparams\u001b[00m\r\n",
      "    │   │   ├── alpha\r\n",
      "    │   │   └── l1_ratio\r\n",
      "    │   └── \u001b[01;34mtags\u001b[00m\r\n",
      "    │       ├── mlflow.project.backend\r\n",
      "    │       ├── mlflow.project.entryPoint\r\n",
      "    │       ├── mlflow.project.env\r\n",
      "    │       ├── mlflow.source.git.commit\r\n",
      "    │       ├── mlflow.source.name\r\n",
      "    │       ├── mlflow.source.type\r\n",
      "    │       └── mlflow.user\r\n",
      "    └── meta.yaml\r\n",
      "\r\n",
      "12 directories, 27 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run `mlflow ui` to show these visually. This will start the MLflow server in http://localhost:5000.\n",
    "\n",
    "```bash\n",
    "$ mlflow ui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-10-29 22:52:05 +0530] [10249] [INFO] Starting gunicorn 19.9.0\n",
      "[2019-10-29 22:52:05 +0530] [10249] [INFO] Listening at: http://127.0.0.1:5000 (10249)\n",
      "[2019-10-29 22:52:05 +0530] [10249] [INFO] Using worker: sync\n",
      "[2019-10-29 22:52:05 +0530] [10260] [INFO] Booting worker with pid: 10260\n",
      "^C\n",
      "[2019-10-29 22:53:00 +0530] [10249] [INFO] Handling signal: int\n",
      "[2019-10-29 22:53:00 +0530] [10260] [INFO] Worker exiting (pid: 10260)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow UI](./images/mlflow-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLmodel\n",
    "\n",
    "The `MLmodel` file allows us to version and share models easily. Below we can see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open ./mlruns/0/ae803fa1a775416e82f4b4be2452c70e/artifacts/model/MLmodel: no such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# !ccat ./mlruns/0/ae803fa1a775416e82f4b4be2452c70e/artifacts/model/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the `MLmodel` keeps track, between others, of\n",
    "\n",
    "- The experiment id, `539b7e5225844c88a764baaa10112f00`\n",
    "- Date \n",
    "- Version of `sklearn` \n",
    "- How the model was stored\n",
    "\n",
    "As we shall see shortly, the pre-packaged Seldon's model server will use this file to serve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Serving\n",
    "\n",
    "To serve this model we will use Seldon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Set up\n",
    "\n",
    "Before anything, we will first set up the `k8s` cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create k8s cluster\n",
    "\n",
    "Firstly, we will create a cluster using [kind](https://kind.sigs.k8s.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"kind\" ...\n",
      " ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n",
      " ✓ Preparing nodes 📦 \n",
      " ✓ Creating kubeadm config 📜 \n",
      " ✓ Starting control-plane 🕹️ \n",
      " ✓ Installing CNI 🔌 \n",
      " ✓ Installing StorageClass 💾 \n",
      "Cluster creation complete. You can now use the cluster with:\n",
      "\n",
      "export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "!kind create cluster\n",
    "!export KUBECONFIG=\"$(kind get kubeconfig-path --name=kind)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then install Helm and a corresponding service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mKubernetes master\u001b[0m is running at \u001b[0;33mhttps://127.0.0.1:34897\u001b[0m\n",
      "\u001b[0;32mKubeDNS\u001b[0m is running at \u001b[0;33mhttps://127.0.0.1:34897/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\u001b[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "$HELM_HOME has been configured at /home/akash/.helm.\n",
      "\n",
      "Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n",
      "\n",
      "Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\n",
      "To prevent this, run `helm init` with the --tiller-tls-verify flag.\n",
      "For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n",
      "Happy Helming!\n",
      "Waiting for deployment \"tiller-deploy\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"tiller-deploy\" successfully rolled out\n",
      "serviceaccount/tiller created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/tiller-cluster-rule created\n",
      "deployment.extensions/tiller-deploy patched\n"
     ]
    }
   ],
   "source": [
    "# !kind get clusters\n",
    "# !export KUBECONFIG=\"$(kind get kubeconfig-path)\"\n",
    "# !echo $KUBECONFIG\n",
    "!kubectl cluster-info\n",
    "!helm init --history-max 200\n",
    "!kubectl rollout status deploy/tiller-deploy -n kube-system\n",
    "!kubectl create serviceaccount --namespace kube-system tiller\n",
    "!kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n",
    "!kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install `seldon-core` on the new cluster using `helm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core\n",
      "LAST DEPLOYED: Tue Oct 29 23:25:42 2019\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Service\n",
      "NAME                                        TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE\n",
      "seldon-operator-controller-manager-service  ClusterIP  10.104.152.235  <none>       443/TCP  0s\n",
      "webhook-server-service                      ClusterIP  10.107.52.253   <none>       443/TCP  0s\n",
      "\n",
      "==> v1/StatefulSet\n",
      "NAME                                DESIRED  CURRENT  AGE\n",
      "seldon-operator-controller-manager  1        1        0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                  READY  STATUS             RESTARTS  AGE\n",
      "seldon-operator-controller-manager-0  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                                   TYPE    DATA  AGE\n",
      "seldon-operator-webhook-server-secret  Opaque  0     0s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME                              SECRETS  AGE\n",
      "seldon-core-seldon-core-operator  1        0s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                         AGE\n",
      "seldondeployments.machinelearning.seldon.io  0s\n",
      "\n",
      "==> v1/ClusterRoleBinding\n",
      "NAME                                 AGE\n",
      "seldon-operator-manager-rolebinding  0s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME           DATA  AGE\n",
      "seldon-config  1     0s\n",
      "\n",
      "==> v1/ClusterRole\n",
      "NAME                          AGE\n",
      "seldon-operator-manager-role  0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n",
      "Waiting for 1 pods to be ready...\n",
      "partitioned roll out complete: 1 new pods have been updated...\n"
     ]
    }
   ],
   "source": [
    "!helm install \\\n",
    "    seldon-core-operator \\\n",
    "    --name seldon-core \\\n",
    "    --repo https://storage.googleapis.com/seldon-charts \\\n",
    "    --namespace seldon-system \\\n",
    "    --set usagemetrics.enabled=true \\\n",
    "    --set ambassador.enabled=true\n",
    "!kubectl rollout status statefulset.apps/seldon-operator-controller-manager -n seldon-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we install `ambassador` which will allow us to reach the Seldon engine in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   ambassador\n",
      "LAST DEPLOYED: Tue Oct 29 23:26:05 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "ambassador  1        1s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                          AGE\n",
      "kubernetesserviceresolvers.getambassador.io   1s\n",
      "authservices.getambassador.io                 1s\n",
      "tracingservices.getambassador.io              1s\n",
      "tlscontexts.getambassador.io                  1s\n",
      "modules.getambassador.io                      1s\n",
      "ratelimitservices.getambassador.io            1s\n",
      "tcpmappings.getambassador.io                  1s\n",
      "kubernetesendpointresolvers.getambassador.io  1s\n",
      "mappings.getambassador.io                     1s\n",
      "consulresolvers.getambassador.io              0s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "ambassador  0s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "ambassador  0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME               TYPE          CLUSTER-IP      EXTERNAL-IP  PORT(S)                     AGE\n",
      "ambassador-admins  ClusterIP     10.104.185.199  <none>       8877/TCP                    0s\n",
      "ambassador         LoadBalancer  10.103.168.82   <pending>    80:30522/TCP,443:30725/TCP  0s\n",
      "\n",
      "==> v1/Deployment\n",
      "NAME        DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "ambassador  3        3        3           0          0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                         READY  STATUS             RESTARTS  AGE\n",
      "ambassador-79744f49fd-d8rqj  0/1    ContainerCreating  0         0s\n",
      "ambassador-79744f49fd-rsvl8  0/1    ContainerCreating  0         0s\n",
      "ambassador-79744f49fd-rvr2f  0/1    ContainerCreating  0         0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "Congratuations! You've successfully installed Ambassador.\n",
      "\n",
      "For help, visit our Slack at https://d6e.co/slack or view the documentation online at https://www.getambassador.io.\n",
      "\n",
      "To get the IP address of Ambassador, run the following commands:\n",
      "NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n",
      "     You can watch the status of by running 'kubectl get svc -w  --namespace default ambassador'\n",
      "\n",
      "  On GKE/Azure:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default ambassador -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
      "\n",
      "  On AWS:\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
      "\n",
      "  echo http://$SERVICE_IP:\n",
      "\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 0 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 1 of 3 updated replicas are available...\n",
      "Waiting for deployment \"ambassador\" rollout to finish: 2 of 3 updated replicas are available...\n",
      "deployment \"ambassador\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!helm install stable/ambassador --name ambassador --set crds.keep=false\n",
    "!kubectl rollout status deployment.apps/ambassador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward port\n",
    "\n",
    "Once the cluster has been created, we need to allow access from the outside to the `ambassador` gateway.\n",
    "One way to do this is to use the `kubectl port-forward` command.\n",
    "In particular, we will forward port `8003` of our local host to the cluster's gateway.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "kubectl \\\n",
    "    port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    8003:8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Seldon Core Analytics\n",
    "\n",
    "Later, after we deploy the models, we will compare their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "For that part to work, we first need to install Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   seldon-core-analytics\n",
      "LAST DEPLOYED: Tue Oct 29 23:27:40 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Job\n",
      "NAME                            DESIRED  SUCCESSFUL  AGE\n",
      "grafana-prom-import-dashboards  1        0           1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "alertmanager-deployment  1        1        1           0          1s\n",
      "grafana-prom-deployment  1        1        1           0          1s\n",
      "prometheus-deployment    1        1        1           0          1s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "prometheus  1s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)       AGE\n",
      "alertmanager              ClusterIP  10.110.189.250  <none>       80/TCP        1s\n",
      "grafana-prom              NodePort   10.100.241.163  <none>       80:31194/TCP  1s\n",
      "prometheus-node-exporter  ClusterIP  None            <none>       9100/TCP      1s\n",
      "prometheus-seldon         ClusterIP  10.101.114.68   <none>       80/TCP        1s\n",
      "\n",
      "==> v1beta1/DaemonSet\n",
      "NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\n",
      "prometheus-node-exporter  1        1        0      1           0          <none>         1s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                      READY  STATUS             RESTARTS  AGE\n",
      "grafana-prom-import-dashboards-5txg5      0/1    ContainerCreating  0         1s\n",
      "alertmanager-deployment-db58649dd-9vpzg   0/1    ContainerCreating  0         1s\n",
      "grafana-prom-deployment-8564b575dd-fgkwd  0/1    ContainerCreating  0         1s\n",
      "prometheus-node-exporter-x9xkw            0/1    Pending            0         1s\n",
      "prometheus-deployment-d57b5c748-wb7lz     0/1    Pending            0         1s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                 TYPE    DATA  AGE\n",
      "grafana-prom-secret  Opaque  1     1s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME                       DATA  AGE\n",
      "alertmanager-server-conf   1     1s\n",
      "grafana-import-dashboards  11    1s\n",
      "prometheus-rules           0     1s\n",
      "prometheus-server-conf     1     1s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "prometheus  1        1s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-analytics --name seldon-core-analytics \\\n",
    "     --repo https://storage.googleapis.com/seldon-charts \\\n",
    "     --set grafana_prom_admin_password=password \\\n",
    "     --set persistence.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Grafana, it will be necessary to forward the port to the respective pod as we did previously to access the Seldon Core deployment.\n",
    "The credentials will be simply `admin` // `password`.\n",
    "\n",
    "This command needs to run constantly on the background, so **please make sure you run it on a separate terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ kubectl port-forward \\\n",
    "    $(kubectl get pods \\\n",
    "        -l app=grafana-prom-server -o jsonpath='{.items[0].metadata.name}') \\\n",
    "    3000:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Deploy models\n",
    "\n",
    "Once the cluster is set up, the next step will to upload these models into a common repository and to deploy two `SeldonDeployment` specs to `k8s`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload models (optional)\n",
    "\n",
    "To make sure our `k8s` pods have access to the models we have just trained using `MLflow`, we will upload them into Google Cloud Storage. Note that to run these commands you need write access into the `gs://seldon-models` bucket and you need to have `gsutil` set up.\n",
    "\n",
    "We will upload both versions of the model to:\n",
    "\n",
    "- `gs://seldon-models/mlflow/model-a`\n",
    "- `gs://seldon-models/mlflow/model-b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp -r mlruns/0/169119a7fe1e4b31a746e891499552b0/artifacts/model gs://seldon-models/mlflow/model-a\n",
    "# !gsutil cp -r mlruns/0/5a6be5a1ef844783a50a6577745dbdc3/artifacts/model gs://seldon-models/mlflow/model-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy specs\n",
    "\n",
    "We will deploy our A/B inference graph to our `k8s` cluster. As we can see below, we will route 50% of the traffic to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-a\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-a\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "  - \u001b[94mgraph\u001b[39;49;00m:\n",
      "      \u001b[94mchildren\u001b[39;49;00m: []\n",
      "      \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "      \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-b\n",
      "      \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "    \u001b[94mname\u001b[39;49;00m: model-b\n",
      "    \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "    \u001b[94mtraffic\u001b[39;49;00m: 50\n",
      "seldondeployment.machinelearning.seldon.io/wines-classifier configured\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./serving/model-a-b.yaml\n",
    "!kubectl apply -f ./serving/model-a-b.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify these have been deployed by checking the pods and `SeldonDeployment` resources in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                READY   STATUS      RESTARTS   AGE\n",
      "alertmanager-deployment-db58649dd-xt5fm             1/1     Running     0          77m\n",
      "ambassador-5c76696fcc-9wvj4                         1/1     Running     0          5h23m\n",
      "ambassador-5c76696fcc-bb96c                         1/1     Running     0          5h23m\n",
      "ambassador-5c76696fcc-mxv8k                         1/1     Running     0          5h23m\n",
      "grafana-prom-deployment-8564b575dd-plchb            1/1     Running     0          77m\n",
      "grafana-prom-import-dashboards-jvnq9                0/1     Completed   0          77m\n",
      "prometheus-deployment-d57b5c748-gr5mf               1/1     Running     0          77m\n",
      "prometheus-node-exporter-c5rdn                      1/1     Running     0          77m\n",
      "wines-classifier-model-a-77efeb1-766f5c8458-548m6   0/2     Running     0          29s\n",
      "wines-classifier-model-a-77efeb1-7c495775cb-dhbq5   2/2     Running     0          74m\n",
      "wines-classifier-model-b-77efeb1-78cc5c94-v6t86     0/2     Running     0          29s\n",
      "wines-classifier-model-b-77efeb1-7b77944d5b-g5wwb   2/2     Running     0          74m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               AGE\n",
      "wines-classifier   74m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get sdep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models\n",
    "\n",
    "We will now run a sample query to test that the inference graph is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \u001b[94m\"data\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"names\"\u001b[39;49;00m: [],\n",
      "        \u001b[94m\"ndarray\"\u001b[39;49;00m: [\n",
      "            \u001b[34m5.550530190667395\u001b[39;49;00m\n",
      "        ]\n",
      "    },\n",
      "    \u001b[94m\"meta\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"metrics\"\u001b[39;49;00m: [],\n",
      "        \u001b[94m\"puid\"\u001b[39;49;00m: \u001b[33m\"ja58a82rrf2amp44rtsl4af7gp\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"requestPath\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"wines-classifier\"\u001b[39;49;00m: \u001b[33m\"seldonio/mlflowserver_rest:0.2\"\u001b[39;49;00m\n",
      "        },\n",
      "        \u001b[94m\"routing\"\u001b[39;49;00m: {},\n",
      "        \u001b[94m\"tags\"\u001b[39;49;00m: {}\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!http \\\n",
    "    --print b \\\n",
    "    localhost:8003/seldon/default/wines-classifier/api/v0.1/predictions \\\n",
    "    data:='{\\\n",
    "        \"names\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"], \\\n",
    "        \"ndarray\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics\n",
    "\n",
    "Now that we have both models running in production, we can analyse their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "To do so, we will iterate over the training set (which can be foud in `./training/wine-quality.csv`), making a request and sending the feedback of the prediction.\n",
    "\n",
    "Since the `/feedback` endpoint requires a `reward` signal (i.e. higher better), we will simulate one as\n",
    "\n",
    "$$\n",
    "  R(x_{n})\n",
    "    = \\begin{cases}\n",
    "        \\frac{1}{(y_{n} - f(x_{n}))^{2}} &, y_{n} \\neq f(x_{n}) \\\\\n",
    "        500 &, y_{n} = f(x_{n})\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    ", where $R(x_{n})$ is the reward for input point $x_{n}$, $f(x_{n})$ is our trained model and $y_{n}$ is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seldon_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8774c69aa90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseldon_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseldon_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeldonClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# sc = SeldonClient(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seldon_core'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    namespace=\"default\",\n",
    "    deployment_name='wines-classifier')\n",
    "\n",
    "df = pd.read_csv(\"./training/wine-quality.csv\")\n",
    "\n",
    "def _get_reward(y, y_pred):\n",
    "    if y == y_pred:\n",
    "        return 500    \n",
    "    \n",
    "    return 1 / np.square(y - y_pred)\n",
    "\n",
    "def _test_row(row):\n",
    "    input_features = row[:-1]\n",
    "    feature_names = input_features.index.to_list()\n",
    "    X = input_features.values.reshape(1, -1)\n",
    "    y = row[-1].reshape(1, -1)\n",
    "    \n",
    "    r = sc.predict(\n",
    "        data=X,\n",
    "        names=feature_names)\n",
    "    \n",
    "    y_pred = r.response.data.tensor.values\n",
    "    reward = _get_reward(y, y_pred)\n",
    "    sc.feedback(\n",
    "        prediction_request=r.request,\n",
    "        prediction_response=r.response,\n",
    "        reward=reward)\n",
    "    \n",
    "    return reward[0]\n",
    "\n",
    "df.apply(_test_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can now access the Grafana dashboard in http://localhost:3000 (credentials are `admin` // `password`). Inside the portal, we will go to the Prediction Analytics dashboard.\n",
    " \n",
    " \n",
    "We can see a snapshot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Seldon Analytics](./images/seldon-analytics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
